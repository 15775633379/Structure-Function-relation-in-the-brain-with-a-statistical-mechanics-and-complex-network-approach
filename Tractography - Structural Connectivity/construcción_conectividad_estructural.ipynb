{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Aqui se presentan las funciones basicas para la extraccion de la tractografia y la extraccion de las conectividades estructurales a partir del tractograma y una parcelacion \"\"\""
      ],
      "metadata": {
        "id": "9ajw9eFIO3kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWS03YESy08S"
      },
      "outputs": [],
      "source": [
        "def tractograma_determinista_PFT(fraw, fbval, fbvec, fmask, ft1):\n",
        "  \"\"\"\n",
        "  Esta función recibe las rutas de los archivos de resonancia magnetica de difusion y retorna un archivo de streamlines extraido por el algoritmo determinista de particle filtering tractografia\n",
        "\n",
        "  Parámetros\n",
        "  ----------\n",
        "  fraw : Ruta al archivo data.nii.gz donde se almacenan los datos de resonancia magnetica de difusion para cada bvec y bval\n",
        "  fbval y fbvec : Rutas a los archivos bvals y bvecs sobre los que se tomaron las imagenes de resonancia magnetica de difusion\n",
        "  fmask : Ruta al archivo brainmask_fs.nii.gz con la mascara de la misma dimension que data.nii.gz para depurar las imagenes de tejidos no cerebrales\n",
        "  ft1 : Ruta al archivo T1w_acpc_dc_restore_brain.nii.gz del cual se generaran las mascaras de tejidos (suele tener una mayor resolucion que data.nii.gz, esta funcion luego redimensiona las mascaras)\n",
        "  \n",
        "  Regresa\n",
        "  -------\n",
        "  Retorna un objeto base para representar las streamlines en cualquier formato (trk, tck, vtk, fib, dpy)\n",
        "  \"\"\"\n",
        "  \n",
        "  #%% Cargar los datos del HCP1200, en este caso con su respectiva matriz affine y el tamaño de los voxeles en la imagen\n",
        "\n",
        "  from dipy.io.image import load_nifti_data, load_nifti, save_nifti\n",
        "\n",
        "  data, affine, voxel_size = load_nifti(fraw, return_voxsize=True) # Subir data de aprox 1gb es computacionalmente exigente y puede se demorado\n",
        "  t1_data, t1_affine, t1_voxel_size = load_nifti(ft1, return_voxsize=True)\n",
        "  mask_data, mask_affine, mask_voxel_size = load_nifti(fmask, return_voxsize=True) \n",
        "\n",
        "  #%% Leer los bvals y bvecs así como crear gtab.\n",
        "\n",
        "  from dipy.core.gradients import gradient_table #, unique_bvals_tolerance, get_bval_indices, round_bvals\n",
        "  from dipy.io.gradients import read_bvals_bvecs\n",
        "\n",
        "  bvals, bvecs = read_bvals_bvecs(fbval, fbvec)\n",
        "  gtab = gradient_table(bvals, bvecs)\n",
        "\n",
        "  #%% Realizar la separación del tejido con la imagen de referencia de alta resolución T1w\n",
        "\n",
        "  from dipy.segment.tissue import TissueClassifierHMRF\n",
        "\n",
        "  nclass = 3 # Número de tejidos para los que obtener máscaras: csf, gm, wm\n",
        "  beta = 0.1 # Suavidad del trazado de las máscaras\n",
        "  hmrf = TissueClassifierHMRF()\n",
        "  initial_segmentation, final_segmentation, PVE = hmrf.classify(t1_data, nclass, beta) # Toma alrededor de 22 iteraciones, puede demorarse\n",
        "\n",
        "  # Máscaras sin redimensionar\n",
        "  csf = np.where(final_segmentation == 1, 1, 0)\n",
        "  gm = np.where(final_segmentation == 2, 1, 0)\n",
        "  wm = np.where(final_segmentation == 3, 1, 0)\n",
        "\n",
        "  #%% Con las máscaras, se procede a redimensionarlas al tamaño de la imagen guía para las parcelas\n",
        "\n",
        "  from dipy.align.reslice import reslice\n",
        "\n",
        "  ds = data.shape\n",
        "  t1s = t1_data.shape\n",
        "  ms = mask_data.shape\n",
        "\n",
        "  t1_new_voxel_size = []\n",
        "  mask_new_voxel_size = []\n",
        "\n",
        "  for i in range(3):\n",
        "      t1_new_voxel_size.append(t1s[i]*t1_voxel_size[i]/ds[i])\n",
        "      mask_new_voxel_size.append(ms[i]*mask_voxel_size[i]/ds[i])\n",
        "\n",
        "  # Las máscaras quedan redimensionadas a la resolución de los datos con su respectiva matriz affine\n",
        "  rwm, rwm_affine = reslice(wm, t1_affine, t1_voxel_size, t1_new_voxel_size)\n",
        "  rgm, rgm_affine = reslice(gm, t1_affine, t1_voxel_size, t1_new_voxel_size)\n",
        "  rcsf, rcsf_affine = reslice(csf, t1_affine, t1_voxel_size, t1_new_voxel_size)\n",
        "\n",
        "  # Se redimensiona la máscara para aplicarla a los datos originales\n",
        "  rmask_data, rmask_affine = reslice(mask_data, mask_affine, mask_voxel_size, mask_new_voxel_size)\n",
        "\n",
        "  #%% Se aplica la máscara redimensionada a los datos \n",
        "\n",
        "  from dipy.segment.mask import applymask\n",
        "\n",
        "  # Nuevos datos de referencia, sin los tejidos ajenos al cerebro\n",
        "  datamask = applymask(data, rmask_data)\n",
        "  datamask_affine = affine\n",
        "  datamask_voxel_size = voxel_size\n",
        "\n",
        "  del data,  # Para vaciar la RAM y permitir el no se agoten los recursos del google colab\n",
        "\n",
        "  #%% Se define el modelo tensorial para aplicar a los voxeles, que recibe como parámetro gtab, es decir, la informacion de los gradientes y constantes importantes relevantes para hallar el tensor\n",
        "\n",
        "  import dipy.reconst.dti as dti\n",
        "\n",
        "  dti_model = dti.TensorModel(gtab)\n",
        "\n",
        "  #%% Generar los picos en cada voxel para la tractografáa \n",
        "\n",
        "  # Se carga la esfera de 362 puntos sobre la que los autovectores asociados a los tensores en cada voxel se proyectaran\n",
        "  from dipy.data import get_sphere\n",
        "  sphere = get_sphere('symmetric362')\n",
        "\n",
        "  from dipy.direction import peaks_from_model\n",
        "\n",
        "  # Se determinan las direcciones o picos de máxima difusión para cada voxel, esta parte puede tardar un rato\n",
        "  peak_indices = peaks_from_model(\n",
        "      model=dti_model, data=datamask, sphere=sphere, relative_peak_threshold=.2,\n",
        "      min_separation_angle=25, mask=rmask_data, npeaks=2)\n",
        "\n",
        "  #%% Generar los criterios para empezar y parar los tractos nerviosos \n",
        "  from dipy.tracking import utils\n",
        "  from dipy.tracking.stopping_criterion import CmcStoppingCriterion\n",
        "\n",
        "  voxel_size_mask = np.average(t1_new_voxel_size)\n",
        "  step_size = 0.5\n",
        "\n",
        "  # Se definen las semillas de la tractografía con la máscara de materia blanca\n",
        "  seeds = utils.seeds_from_mask(rwm, datamask_affine, density=1)\n",
        "\n",
        "  # Se define el Stopping Criterion con las máscaras de los tres tejidos rwm, rgm, rcsf\n",
        "  cmc_criterion = CmcStoppingCriterion.from_pve(rwm,\n",
        "                                                rgm,\n",
        "                                                rcsf,\n",
        "                                                step_size=step_size,\n",
        "                                                average_voxel_size=voxel_size_mask)\n",
        "\n",
        "  #%%  Se define la Particle Filtering Tractography, enviando como parámetro las semillas, el StoppingCriterion y los picos para cada voxel\n",
        "\n",
        "  from dipy.tracking.local_tracking import (LocalTracking,ParticleFilteringTracking)\n",
        "\n",
        "  pft_streamline_generator = ParticleFilteringTracking(peak_indices,\n",
        "                                                      cmc_criterion,\n",
        "                                                      seeds,\n",
        "                                                      datamask_affine,\n",
        "                                                      max_cross=1,\n",
        "                                                      step_size=step_size,\n",
        "                                                      maxlen=1000,\n",
        "                                                      pft_back_tracking_dist=2,\n",
        "                                                      pft_front_tracking_dist=1,\n",
        "                                                      particle_count=15,\n",
        "                                                      return_all=False)\n",
        "  \n",
        "  #%% Definido el tipo de tractografía, con Streamlines se generan las curvas como se especificó arriba\n",
        "\n",
        "  from dipy.tracking.streamline import Streamlines\n",
        "\n",
        "  pft_streamlines = Streamlines(pft_streamline_generator)\n",
        "\n",
        "  #%% Guardar tractograma en archivo trk con el formato canónico RAS+ en mm\n",
        "\n",
        "  from dipy.io.stateful_tractogram import Space, StatefulTractogram\n",
        "  from dipy.io.streamline import save_tractogram\n",
        "\n",
        "  # Para generar el tractograma pft_sft es necesario enviar como parámetro el archivo NIFTI de donde se obtuvieron los tractos\n",
        "  data_img = nib.load(fraw)\n",
        "\n",
        "  # Se genera el tractograma en el espacio RASMM\n",
        "  pft_sft = StatefulTractogram(pft_streamlines, data_img, Space.RASMM)\n",
        "\n",
        "  # Se guarda el tractograma en formato trk, este se puede visualizar con el programa TrackVis o cargandolo a python como se verá más adelante\n",
        "\n",
        "  return pft_sft\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conectividades_estructurales(patient, streamlines, parcelacion, par_data, par_affine, dpar):\n",
        "  \"\"\"\n",
        "  Esta función recibe los datos del paciente, sus streamlines, y los datos de la parcelacion a usar, guardando las matrices estructurales producidas\n",
        "\n",
        "  Parámetros\n",
        "  ----------\n",
        "  patient, streamlines : Cadena de texto (string) que identifica al paciente (patient), junto con sus streamlines\n",
        "  streamlines : Streamlines asociadas al sujeto\n",
        "  parcelacion, par_data, par_affine : Cadena de texto (string) que identifica la parcelacion usada, junto con su respectivo archivo par_data donde se almacenan los labels que identifican las parcelas y la transformacion par_affine\n",
        "  dpar : Ruta a la carpeta donde se guardaran las matrices estructurales generadas\n",
        "\n",
        "  Regresa\n",
        "  -------\n",
        "  Retorna tres archivos .npy que contienen la diferentes matrices estructurales normalizadas al valor de la entrada maxima, en terminos del conteo de tractos o la longitud promedio de los tractos entre dos parcelas\n",
        "\n",
        "  sum : Cada entrada representa el numero de tractos que conectan dos parcelas\n",
        "  len : Cada entrada representa la longitud promedio de los tractos que conectan dos parcelas\n",
        "  invlen : Cada entrada representa el inverso de la longitud promedio de los tractos que conectan dos parcelas\n",
        "  \"\"\"\n",
        "\n",
        "  #%% Con los datos del tractograma se procede a crear la matriz con la parcela elegida anteriormente, esto se hará por medio el conteo de tractos que van de una parcela a otra, definiendo un link pesado\n",
        "  from dipy.tracking import utils\n",
        "\n",
        "  # Se extrae la matriz de conectividad M pasando la matriz affine y la imagen de la parcelación correspondiente\n",
        "  preM, grouping = utils.connectivity_matrix(streamlines, par_affine, label_volume=par_data.astype(int), return_mapping=True, mapping_as_streamlines=False)\n",
        "\n",
        "  # Redimensionar la matriz estructural sin el nodo empty con label cero\n",
        "  M = preM[1:len(preM),1:len(preM)]\n",
        "\n",
        "  #%% En esta parte se utiliza el diccionario grouping para generar la matriz structural proporcional al promedio de la longitud de los tractos que unen una parcela con la otra\n",
        "\n",
        "  # Crea una lista donde cada posicion corresponde a la longitud lenghts(i) del tracto de label i\n",
        "  from dipy.tracking.utils import length\n",
        "\n",
        "  lengths = list(length(streamlines))\n",
        "\n",
        "  # Se crea un ndarray de ceros con el mismo tamaño de M para guardar los datos\n",
        "  M_len, M_invlen = np.zeros(M.shape), np.zeros(M.shape)\n",
        "\n",
        "  # Se recoren los posibles links para calcular en cada uno la longitud promedio de todos los tractos\n",
        "  for i in range(0, M.shape[0]):\n",
        "    for j in range(i, M.shape[1]):\n",
        "\n",
        "      # index: contiene los indices para acceder a los tractos que conectan dos parcelas en el vector de longitudes\n",
        "      index = grouping[i,j]\n",
        "      # longitud: es el numero de tractos que conectan dos parcelas\n",
        "      longitud = 1.0*len(index)\n",
        "      # sum: será la variable que contendra la suma total de la longitud de los tractos que conectan dos parcelas\n",
        "      sum = 0.0\n",
        "\n",
        "      # Si no hay tractos (longitud = 0.0) entre dos parcelas el valor en la matriz será cero, de lo contrario será el promedio de las longitudes o su inversa\n",
        "      if longitud == 0.0:\n",
        "\n",
        "        # Se llena la matriz de adyacencia para el caso donde no existan tractos que conecten las dos regiones\n",
        "        M_len[i,j], M_len[j,i] = 0.0, 0.0\n",
        "        M_invlen[i,j], M_invlen[j,i] = 0.0, 0.0\n",
        "\n",
        "      # Si hay tractos se proceden a calcular la matriz de longitudes 1/longitudes\n",
        "      else:\n",
        "\n",
        "        # Como se mencionó antes, sum se usa para sumar las longitudes de los tractos entre dos parcelas\n",
        "        for k in index:\n",
        "          sum += lengths[k]\n",
        "        # Con sum y longitud se calcula el promedio de la longitudes de los tractos que conectan dos parcelas\n",
        "        prom = sum/longitud\n",
        "        # Se llena la matriz de adyacencia con el valor promedio y 1/promedio de la longitud\n",
        "        M_len[i,j], M_len[j,i] = prom, prom\n",
        "        M_invlen[i,j], M_invlen[j,i] = 1.0/prom, 1.0/prom\n",
        "\n",
        "  #%% Se guardan las matrices normalizando con el maximo valor\n",
        "  from os.path import expanduser, join\n",
        "\n",
        "  name = 'EST' + '_sum_no_norm_' + patient + parcelacion + '.npy'\n",
        "  festconnectivity = join(dpar, name)\n",
        "  np.save(festconnectivity, M)\n",
        "\n",
        "  M = M/M.max()\n",
        "  name = 'EST' + '_sum_' + patient + parcelacion + '.npy'\n",
        "  festconnectivity = join(dpar, name)\n",
        "  np.save(festconnectivity, M)\n",
        "\n",
        "  M_len = M_len/M_len.max()\n",
        "  name = 'EST' + '_len_' + patient + parcelacion + '.npy'\n",
        "  festconnectivity = join(dpar, name)\n",
        "  np.save(festconnectivity, M_len)\n",
        "\n",
        "  M_invlen = M_invlen/M_invlen.max()\n",
        "  name = 'EST' + '_invlen_' + patient + parcelacion + '.npy'\n",
        "  festconnectivity = join(dpar, name)\n",
        "  np.save(festconnectivity, M_invlen)\n",
        "  "
      ],
      "metadata": {
        "id": "MXvm5rF219DH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}