{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18640,"status":"ok","timestamp":1656395346083,"user":{"displayName":"Luis Higuera","userId":"06864963932410557035"},"user_tz":300},"id":"ADlRJCxfPkVM","outputId":"efaec025-8a7b-47f1-a595-c48c5c4b45d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting nilearn\n","  Downloading nilearn-0.9.1-py3-none-any.whl (9.6 MB)\n","\u001b[K     |████████████████████████████████| 9.6 MB 5.7 MB/s \n","\u001b[?25hRequirement already satisfied: joblib>=0.15 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.1.0)\n","Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.21.6)\n","Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.0.2)\n","Requirement already satisfied: requests>=2 in /usr/local/lib/python3.7/dist-packages (from nilearn) (2.23.0)\n","Requirement already satisfied: nibabel>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from nilearn) (3.0.2)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from nilearn) (4.2.6)\n","Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.3.5)\n","Collecting scipy>=1.5\n","  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n","\u001b[K     |████████████████████████████████| 38.1 MB 1.3 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->nilearn) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->nilearn) (2022.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0->nilearn) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (2.10)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->nilearn) (3.1.0)\n","Installing collected packages: scipy, nilearn\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.4.1\n","    Uninstalling scipy-1.4.1:\n","      Successfully uninstalled scipy-1.4.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed nilearn-0.9.1 scipy-1.7.3\n"]}],"source":["pip install nilearn"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1368,"status":"ok","timestamp":1656395347448,"user":{"displayName":"Luis Higuera","userId":"06864963932410557035"},"user_tz":300},"id":"ZfnykthLzUKE","outputId":"6ea09ff8-bf68-44c2-a1d4-5a27f62e3f9b"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/nilearn/input_data/__init__.py:27: FutureWarning: The import path 'nilearn.input_data' is deprecated in version 0.9. Importing from 'nilearn.input_data' will be possible at least until release 0.13.0. Please import from 'nilearn.maskers' instead.\n","  warnings.warn(message, FutureWarning)\n"]}],"source":["#Para usarlas es necesario instalar e importar las librerias networkx, numpy y nilearn\n","import numpy as np\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","from nilearn import connectome\n","import pandas as pd\n","from nilearn.input_data import NiftiLabelsMasker\n","from nilearn import connectome\n","from nilearn.input_data import NiftiLabelsMasker\n","import nilearn\n","import numpy as np\n","\n","def conectividad_mutual_entropy(timeseries):\n","  l = timeseries\n","  trans = list(map(list, zip(*l)))\n","  # creemos la matriz de correlaciones con nilearn\n","  from nilearn import connectome\n","  #creamos la función para correlación\n","  correlation_measure = connectome.ConnectivityMeasure(kind='correlation')\n","\n","  #creamos las matrices usando la función\n","  trans = np.array(trans) #hagamos un array de la lista donde estan las series de tiempo transpuestas\n","  simulation_matrix = correlation_measure.fit_transform([trans])\n","  simulation_matrix = simulation_matrix[0]\n","\n","  #hagamos la diagonal 0\n","  for i in range(0,len(simulation_matrix)):\n","    simulation_matrix[i][i] = 0\n","  return simulation_matrix\n","\n","def metropolis_hastings(M, iteraciones, reinicios, T, acople, resistencia, high = 1.0, low = -1.0):\n","\n","  graf = nx.convert_matrix.from_numpy_array(M, parallel_edges=False, create_using = nx.DiGraph())\n","  graf.remove_edges_from(nx.selfloop_edges(graf))\n","\n","  # Array en el que se depositan las series de tiempo de los nodos\n","  time_series = np.zeros((len(graf.nodes()), reinicios*iteraciones))\n","  # Array en el que se depositan las energias asociadas a cada estado de la red en cada iteracion\n","  E = np.zeros(reinicios*iteraciones)\n","\n","  # Un vector donde se seleccionan aleatoriamente los nodos a los que se les cambiara su estado\n","  n = np.random.choice(graf.nodes(), size = reinicios*iteraciones)\n","  # Vector de numeros aleatorios para \n","  number = np.random.rand(reinicios*iteraciones)\n","  \n","  for i in graf.nodes():\n","    graf.nodes[i]['state'] = high if np.random.rand() < 0.5 else low\n","\n","  for reinicio in range(reinicios):\n","  \n","    # Se crea un valor de spin para cada nodo aleatoriamente (condiciones iniciales)\n","    for i in graf.nodes():\n","        #graf.nodes[i]['state'] = high if np.random.rand() < 0.5 else low\n","        graf.nodes[i]['state'] = high if graf.nodes[i]['state'] == low else low\n","\n","    energia = 0.0\n","    for nodes in graf.nodes():\n","      energia_nodo = graf.nodes[nodes]['state']*(resistencia[nodes]/2.0) -(acople/2.0)*graf.nodes[nodes]['state']*np.sum(graf.nodes[vecinos]['state']*M[nodes,vecinos] for vecinos in graf.predecessors(nodes))\n","      energia += energia_nodo\n","\n","    for i in range(iteraciones):\n","\n","      estado_previo = graf.nodes[n[reinicio*iteraciones + i]]['state']\n","\n","      graf.nodes[n[reinicio*iteraciones + i]]['state'] = high if estado_previo == low else low \n","\n","      energia_cambio = 0.0\n","      for nodes in graf.nodes():\n","        energia_nodo = graf.nodes[nodes]['state']*(resistencia[nodes]/2.0) -(acople/2.0)*graf.nodes[nodes]['state']*np.sum(graf.nodes[vecinos]['state']*M[nodes,vecinos] for vecinos in graf.predecessors(nodes))\n","        energia_cambio += energia_nodo\n","\n","      de = energia_cambio - energia\n","\n","      if de < 0:\n","        energia = energia_cambio\n","      else:\n","        p = np.exp(-de/T)\n","        if number[reinicio*iteraciones + i] < p:\n","          energia = energia_cambio\n","        else:\n","          graf.nodes[n[reinicio*iteraciones + i]]['state'] = estado_previo\n","\n","      E[reinicio*iteraciones + i] = energia\n","\n","      for k in graf.nodes():\n","        time_series[k, reinicio*iteraciones + i] = graf.nodes[k][\"state\"]\n","\n","  return E, time_series\n","\n","def calculo_entropia(energias,particiones):\n","  rango = max(energias) - min(energias)\n","  ancho_intervalo = rango/particiones\n","  punto_inicio = min(energias)\n","  frecuencias = []\n","  for i in range(particiones):\n","    y = 0 \n","    for x in energias:    \n","      a = punto_inicio + (i*ancho_intervalo)\n","      b= punto_inicio +  (i+1)*ancho_intervalo\n","      if i == particiones:\n","        if x >=  a and x<= b:\n","          y +=1\n","      if i != particiones:\n","        if x >=  a and x< b:\n","          y +=1\n","    frecuencias.append(y)\n","    \n","  suma_entropia = 0\n","  for i in frecuencias:\n","    probabilidad = i/len(energias)\n","    if probabilidad != 0:\n","      suma_entropia -= probabilidad*np.log(probabilidad)\n","  return suma_entropia"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32673,"status":"ok","timestamp":1656395380119,"user":{"displayName":"Luis Higuera","userId":"06864963932410557035"},"user_tz":300},"id":"SR_4wzQeSFA3","outputId":"5955b2a2-a379-48e5-a8fa-81ab78314d07"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n","/gdrive\n"]}],"source":["#%% Conectar con google drive, donde se encuentran los datos HCP1200 requeridos\n","from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2042,"status":"ok","timestamp":1656395382158,"user":{"displayName":"Luis Higuera","userId":"06864963932410557035"},"user_tz":300},"id":"Sciyru4ojMzg","outputId":"0b58a95d-027b-46b7-8345-0d49eb42c90d"},"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/Shareddrives/GENSyR /CNF (Complejidad y NeuroFísica)/Carpeta de trabajo/Tesis Juan y Diego /Datos Poblaciones/Sujetos Tesis\n"]}],"source":["cd /gdrive/Shareddrives/GENSyR /CNF (Complejidad y NeuroFísica)/Carpeta de trabajo/Tesis Juan y Diego /Datos Poblaciones/Sujetos Tesis"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1124,"status":"ok","timestamp":1656395383279,"user":{"displayName":"Luis Higuera","userId":"06864963932410557035"},"user_tz":300},"id":"qEut0MSTjgwJ"},"outputs":[],"source":["sujetos = ['Emotion_F-']\n","estructurales = []\n","for sujeto in sujetos:\n","  estructural = np.load(sujeto+ '/EST_sum_no_norm_' + sujeto + '.npy')\n","  estructural = estructural/np.max(estructural)\n","  estructural = estructural*(estructural > 0.0*np.max(estructural))\n","  estructurales.append(estructural)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AqOc7jSmlZRG","executionInfo":{"status":"ok","timestamp":1656423069547,"user_tz":300,"elapsed":27686270,"user":{"displayName":"Luis Higuera","userId":"06864963932410557035"}},"outputId":"0fd6c29f-b6f9-4e35-dac9-e3c1fa693f1d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:69: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:69: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:69: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:69: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:69: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:69: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:69: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:69: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"]}],"source":["#Acá se va a hallar el parámetro de temperatura que nos maximiza la entropía\n","entropias = []\n","energias = []\n","temperaturas = []\n","SIMS = []\n","for matriz in estructurales:\n","  M = matriz\n","  resistencias = np.ones(M.shape[0])\n","  Temperatura1 = 3\n","  Temperatura2 = 5\n","\n","  E1, time_series1 = metropolis_hastings(M, iteraciones = 500, reinicios = 1100, T = Temperatura1, acople = 1.0, resistencia = resistencias, high = 1.0, low = -1)\n","\n","  E2, time_series2 = metropolis_hastings(M, iteraciones = 500, reinicios = 1100, T = Temperatura2, acople = 1.0, resistencia = resistencias, high = 1.0, low = -1)\n","\n","  for i in range(8):\n","    entropia1 = calculo_entropia(E1,100)\n","    entropia2 = calculo_entropia(E2,100)\n","\n","    if entropia1 < entropia2:\n","      Temperatura1 = (Temperatura1 + Temperatura2)/2\n","      E1, time_series1 = metropolis_hastings(M, iteraciones = 500, reinicios = 1100, T = Temperatura1, acople = 1.0, resistencia = resistencias, high = 1.0, low = -1)\n","\n","    else:\n","      Temperatura2 = (Temperatura1+Temperatura2)/2\n","      E2, time_series2 = metropolis_hastings(M, iteraciones = 500, reinicios = 1100, T = Temperatura2, acople = 1.0, resistencia = resistencias, high = 1.0, low = -1)\n","\n","    time_series1 = time_series1[:,30000:len(time_series2[4])]\n","    SIM = conectividad_mutual_entropy(time_series1)\n","    SIMS.append(SIM)\n","    entropias.append(entropia1)\n","    energias.append(E1)\n","    temperaturas.append(Temperatura1)\n","    temperaturas.append(Temperatura2)\n","\n","temperaturas = np.array(temperaturas)\n","SIMS = np.array(SIMS)\n","energias = np.array(energias)\n","entropias = np.array(entropias)\n","\n","np.save(sujetos[0] + '/temperaturas_PE_' + sujetos[0] + '.npy',temperaturas)\n","np.save(sujetos[0] + '/SIM_PE_' + sujetos[0] + '.npy',SIMS)\n","np.save(sujetos[0] + '/entropia_' + sujetos[0] +'_.npy',entropias)\n","np.save(sujetos[0] +'/energias_' + sujetos[0] +'_.npy',energias)\n","temp = np.array([Temperatura1,Temperatura2])\n","np.save(sujetos[0] +'/temperaturas_' + sujetos[0] + '_.npy',temp)\n","\n","  "]},{"cell_type":"code","execution_count":7,"metadata":{"id":"u0xgqzh6OgAL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656423071011,"user_tz":300,"elapsed":1474,"user":{"displayName":"Luis Higuera","userId":"06864963932410557035"}},"outputId":"386db5c4-0a5b-44f7-b84c-ff721757de80"},"outputs":[{"output_type":"stream","name":"stdout","text":["SIMULADO\n","rest 50.19766998970369\n","wm 35.14714767153205\n","social 46.129359541030134\n","emocion 36.94918119478083\n","ESTRUCTURAL\n","rest 50.67030814734848\n","wm 35.48175526577145\n","social 46.57098828865569\n","emocion 37.29192373939877\n","[3.0234375 3.03125  ]\n","[3.96282842 3.96282842 3.96282842 3.96282842 3.96282842 3.96282842\n"," 3.96282842 3.95664276]\n"]}],"source":["rest = np.load(sujetos[0] + '/REST1_PE_' + sujetos[0] + '.npy')\n","wm = np.load(sujetos[0] + '/WM_PE_' + sujetos[0] + '.npy')\n","social = np.load(sujetos[0] + '/SOCIAL_PE_' + sujetos[0] + '.npy')\n","emocion = np.load(sujetos[0] + '/EMOTION_PE_' + sujetos[0] + '.npy')\n","\n","print('SIMULADO')\n","print('rest',np.linalg.norm(SIM - rest))\n","print('wm',np.linalg.norm(wm - SIM))\n","print('social', np.linalg.norm(social - SIM) )\n","print('emocion',np.linalg.norm(emocion - SIM))\n","\n","print('ESTRUCTURAL')\n","print('rest',np.linalg.norm(estructurales[0] - rest))\n","print('wm',np.linalg.norm(wm - estructurales[0]))\n","print('social',np.linalg.norm(social - estructurales[0]))\n","print('emocion',np.linalg.norm(emocion - estructurales[0]))\n","\n","print(temp)\n","print(entropias)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ZPZHgpzKOp0g","executionInfo":{"status":"ok","timestamp":1656423071012,"user_tz":300,"elapsed":9,"user":{"displayName":"Luis Higuera","userId":"06864963932410557035"}}},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Emotion_F-.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}